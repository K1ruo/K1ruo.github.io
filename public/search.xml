<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python爬虫笔记(③)]]></title>
    <url>%2F2020%2F03%2F12%2Fpython%E7%88%AC%E8%99%AB(%E2%91%A2)%2F</url>
    <content type="text"><![CDATA[信息的处理与爬虫实例##信息的标记方法示例 处理示例：步骤：搜索标签，提取标签中的对应数据（tag处理和字典处理方法相同）tag.get(‘key’)bs对象的find_all方法(name,attrs,recursive,string,**kwargs) name 标签名检索字符串，字符或列表 attrs：标签属性值的检索，find_all(‘p’,’course’)检索 p标签，或不设置标签 recursive: 是否对子孙全部检索，默认为true string ：检索字符串.find_all()==() 爬虫实例：中国大学排名步骤：①.获得排名网络内容。②.提取过滤内容。③输出结果 12345678910111213141516171819202122232425262728293031323334import requestsfrom bs4 import BeautifulSoupimport bs4def gethtml(url): #获得网页内容 try: r=requests.get(url) r.raise_for_status r.encoding=r.apparent_encoding#转换编码 return r.text except: print(&apos;error&apos;)def fill(ulist,html): #处理网页内容 soup=BeautifulSoup(html,&apos;html.parser&apos;) for tr in soup.find(&apos;tbody&apos;).children: if isinstance(tr,bs4.element.Tag):#存在非tag标签，过滤非tag标签 tds=tr(&apos;td&apos;) ulist.append([tds[0].string,tds[1].string,tds[2].string,tds[3].string])def printinfo(ulist,num):#处理输出格式 mask=&apos;&#123;0:^10&#125;\t&#123;1:&#123;4&#125;^10&#125;\t&#123;2:^10&#125;\t&#123;3:^10&#125;&apos; print(mask.format(&apos;排名&apos;,&apos;名称&apos;,&apos;位置&apos;,&apos;分数&apos;,chr(12288))) for i in range(num): u=ulist[i] print(mask.format(u[0],u[1],u[2],u[3],chr(12288))) def main(): uinfo=[] url=&apos;http://www.zuihaodaxue.com/Greater_China_Ranking2019_0.html&apos; html=gethtml(url) fill(uinfo,html) printinfo(uinfo,20)if __name__==&apos;__main__&apos;: main()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫笔记(②)]]></title>
    <url>%2F2020%2F03%2F12%2Fpython%E7%88%AC%E8%99%AB(%E2%91%A1)%2F</url>
    <content type="text"><![CDATA[网站图片爬取示例1234567891011121314151617import requestsimport osurl=&apos;http://www.example.com/200001021.jpg&apos;root=&apos;d:/pics&apos; #设置文件存储位置path=root+url.split(&apos;/&apos;)[-1] #取图片名称try: if not os.path.exists(root): os.mkdir(root) if not os.path.exists(path): r=requests.get(url) with open(path,&apos;wb&apos;) as f: #二进制保存 f.write(r.content) print(&apos;ok&apos;) else: print(&apos;already exist&apos;)except: print(&apos;error&apos;) Beautifulsoup模块beautifulsoup模块使用： 12from bs4 importBeautifulSoupsoup = BeautifulSoup(&apos;&lt;p&gt;data&lt;/p&gt;&apos;,&apos;html.parser&apos;) 库解析器html.parserlxmlxmlhtml5lib tag标签 soup.,默认返回第一个tag name tag的名字，通过.name获取，字符串类型 attrs 标签的属性，字典形式组织，.attrs，举例，将标签元素拆解成字典 string 返回&lt;&gt;&lt;/&gt;中的字符串，格式：.string comment 返回标签内字符串的注释部分 ，中的内容，格式：.comment 遍历方式 下行遍历： .contents,子节点的列表，把所有的儿子节点放入列表.children ，子节点的迭代类型，遍历儿子节点.descendants，子孙节点的迭代类型，循环遍历子孙节点 1234for child in soup.body.children: #遍历儿子节点 print(child)for child in soup.body.descendants:#遍历子孙节点 print(child) 上行遍历.parents 节点的父标签.parents 节点的父标签的迭代类型 平行遍历 prettify()方法，将返回的内容添加’换行‘，显示更美观]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫笔记(①)]]></title>
    <url>%2F2020%2F03%2F11%2Fpython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[python爬虫笔记记录request库：方法：request，get，head，post，put，patch，deleterequests.get(url,params=None,** kwargs)url：外链params： url额外参数，字典或字节流可选**kwargs：12个控制访问的参数 response对象属性：r.status_code : HTTP请求的返回状态，200表示连接成功，404表示失败r.text : HTTP响应内容的字符串形式，即，url对应的页面内容r.encoding: 从HTTP header中猜测的响应内容编码方式r.apparent_encoding: 从内容中分析出的响应内容编码方式（备选编码方式）r.content: HTTP响应内容的二进制形式 r.encoding：如果header中不存在charset，则认为编码为ISO‐8859‐1r.text根据r.encoding显示网页内容r.apparent_encoding：根据网页内容分析出的编码方式 可以看作是r.encoding的备选 requests异常requests.ConnectionError:网络连接错误异常，如DNS查询失败、拒绝连接等 requests.HTTPError:HTTP错误异常requests.URLRequired:URL缺失异常requests.TooManyRedirects:超过最大重定向次数，产生重定向异常requests.ConnectTimeout:连接远程服务器超时异常requests.Timeou:t请求URL超时，产生超时异常 r.raise_for_status():如果不是200，产生异常requests.HTTPError requests库的7个方法：requests.request()， requests.get()，requests.head()， requests.post()， requests.put()，requests.patch()， requests.delete(） http的六种请求方法：get,post,head,put,patch,deletepatch可以做到更新部分，对比put，节约网络带宽 方法示例：head()方法 12345&gt;&gt;&gt;r=requests.head(&apos;http://httpbin.org/get&apos;) &gt;&gt;&gt;r.headers &#123;&apos;Content‐Length&apos;: &apos;238&apos;, &apos;Access‐Control‐Allow‐Origin&apos;: &apos;\*&apos;, &apos;AccessControl‐Allow‐Credentials&apos;: &apos;true&apos;, &apos;Content‐Type&apos;: &apos;application/json&apos;, &apos;Server&apos;: &apos;nginx&apos;, &apos;Connection&apos;: &apos;keep‐alive&apos;, &apos;Date&apos;: &apos;Sat, 18 Feb 2017 12:07:44 GMT&apos;&#125; &gt;&gt;&gt;r.text &apos;&apos; post方法(post字典) 1234567&gt;&gt;&gt; payload=&#123;&apos;key1&apos;: &apos;value1&apos;,&apos;key2&apos;:&apos;value2&apos;&#125; &gt;&gt;&gt; r=requests.post(&apos;http://httpbin.org/post&apos;,data= payload)&gt;&gt;&gt; print(r.text)&gt;&gt;&gt; &#123; ... &quot;form&quot;: &#123; &quot;key2&quot;: &quot;value2&quot;, &quot;key1&quot;: &quot;value1&quot; &#125;,&#125; post方法(post字符串) 123456&gt;&gt;&gt; r=requests.post(&apos;http://httpbin.org/post&apos;,data= &apos;ABC&apos;) &gt;&gt;&gt; print(r.text) &#123; ... &quot;data&quot;: &quot;ABC&quot; &quot;form&quot;: &#123;&#125;, &#125; put方法，与post类似 requests.request(method,url,** kwargs)method : 请求方式，对应get/put/post等7种 ∙url: 拟获取页面的url链接 ∙**kwargs: 控制访问的参数，共13个 1&gt;&gt;&gt; kv=&#123;&apos;key1&apos;:&apos;value1&apos;,&apos;key2&apos;:&apos;value2&apos;&#125; &gt;&gt;&gt; r=requests.request(&apos;GET&apos;,&apos;http://python123.io/ws&apos;,params=kv) &gt;&gt;&gt; print(r.url) http://python123.io/ws?key1=value1&amp;key2=value2 ** kwargs: 控制访问的参数，均为可选项 params: 字典或字节序列，作为参数增加到url中data : 字典、字节序列或文件对象，作为Request的内容json: JSON格式的数据，作为Request的内容headers : 字典，HTTP定制头cookies:字典或cookiejar,request中的cookieauth:元组，支持http认证功能files:字典类型，传输文件timeout:设定超时时间，秒为单位proxies:字典类型，设置代理服务器allow_redirects: True/False，默认为True，重定向开关stream : True/False，默认为True，获取内容立即下载开关verify : True/False，默认为True，认证SSL证书开关cert : 本地SSL证书路径 函数参数requests.request(method,url,kwargs)requests.get(url,params=None, kwargs)requests.head(url,kwargs)requests.post(url,data=None, json=None, kwargs)requests.put(url,data=None, kwargs)requests.patch(url,data=None, kwargs)requests.delete(url,kwargs)** url: 拟更新页面的url链接 ∙ data : 字典、字节序列或文件，Request的内容 json: JSON格式的数据，Request的内容 ∙ **kwargs: 12个控制访问的参数]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hackbar破解]]></title>
    <url>%2F2020%2F01%2F19%2Fhackbar%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[hackbar插件的破解&#160;&#160; &#160; &#160;hackbar作为web安全爱好者常用的必备工具之一，集合这场用的各种小功能，但新版的hackbar却已经成为付费激活的插件，而其余复刻hackbar的插件用起来用很不舒服，所以在这里带来了火狐，谷歌两个浏览器hackbar插件的免费使用方式。 火狐浏览器的方法&#160; &#160;首先火狐插件商店里面可以搜到hackbar的复刻版max hackbar，可以直接使用。&#160; &#160;另一种方法就是使用之前并没有收费的版本，并关闭掉自动更新即可。&#160; &#160;在这里举例2.1.3版本的安装，github地址：https://github.com/Mr-xn/hackbar2.1.3 直接按照步骤安装即可，同时关闭掉自动更新。 谷歌浏览器的方法&#160; &#160;谷歌浏览器的hackbar可直接进行破解，在安装插件完成后，找到插件的安装位置，以mac举例，安装位置在user/xxx/library/Application Support/Google/Chrome/Default/Extensions win自行百度或者直接文件搜索即可。进入拓展文件夹后，自行判断哪个是hackbar安装文件，（可通过判断插件id或者观察文件图片分别），在安装文件夹js里面找到hackbar-panel.js文件，将此处的false改成true。 &#160; &#160;在修改代码后，会提示插件损坏无法正常使用。然后我们需要重新打包程序，然后再离线安装即可。&#160; &#160;步骤如下：首先打开插件界面，然后打开开发这模式，点击打包拓展程序。打包完成后在插件的根目录即可找到生成的crx后缀文件。然后我们需要将此文件后缀修改为rar，全部解压成文件夹。在浏览器拓展程序位置打开开发者模式，然后再点击加载，选择刚刚解压出的文件夹即可。]]></content>
      <categories>
        <category>插件</category>
      </categories>
      <tags>
        <tag>破解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sudo误配提权]]></title>
    <url>%2F2020%2F01%2F18%2Fsudo%E8%AF%AF%E9%85%8D%E6%8F%90%E6%9D%83%2F</url>
    <content type="text"><![CDATA[linux提权之sudo误配提权&#160; &#160; &#160; &#160;此文章思路来自于freebuf，可直接搜索【全程带阻：记一次授权网络攻防演练】查看 &#160; &#160; &#160; &#160;Linux提权方式有很多，例如利用内核栈溢出提权、搜寻配置文件中的明文密码、环境变量劫持高权限程序、不安全的服务、借助权能（POSIX capabilities）提权、sudo 误配、SUID 滥用等等。 &#160; &#160; &#160; &#160;今天举例sudo误配提权，此提权方式网上存在靶机JIS-CTF-VulnUpload-CTF01，sudo误配提权的一种方式为：查看 home​/ 目录是否 存在.sudo_as_admin_successful 文件，若有则可以输入当前低权账号的密码直接 sudo su 切换为 root 用户。而低权限用户的账户密码的获取可通过翻找各类配置文件获得（简易获得方式）。]]></content>
      <categories>
        <category>Linux提权</category>
      </categories>
      <tags>
        <tag>提权</tag>
      </tags>
  </entry>
</search>
